# Cross correlation and Perceptron layers.

Complete neural networks are a set of layers, in which first there are the convolution layers, followed by a layer that sequences the output of the convolution layers called the flattern, and finally the perceptron neural network layers.

For exmple Lenet 5:
https://medium.com/@siddheshb008/lenet-5-architecture-explained-3b559cb2d52b

From: https://en.wikipedia.org/wiki/AlexNet
For example LeNet and alexNet:
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/480px-Comparison_image_neural_networks.svg.png" alt="Image of a neuron"/>

Now we know how works a CNN, but I dont know how the kernels become from random numbers to a features filter.
